{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle as pkl\n",
    "from re import search\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input files\n",
    "STRING_DATA  = 'data/STRING/9606.protein.links.full.v11.0.txt'\n",
    "HURI_DATA    = 'data/HuRI/HuRI.tsv'\n",
    "PARALOG_DATA = 'data/DGD/duplicate_genes_Hsapiens.tsv'\n",
    "\n",
    "# Output files\n",
    "# Make sure all the folders are in place\n",
    "PANTHER_PARALOGS = 'data/PANTHER/paralogs-GeneID'  # see uniprot_enrez.csv in supp.mat.\n",
    "GENE_INFO        = 'data/NCBI/gene_map.csv'\n",
    "SIGNOR_DATA      = 'data/SIGNOR/geneID_interactions.pkl'\n",
    "ENSP_ENTREZ_MAP  = 'data/ens_entrez_maps/ensp_entrez_mapping.pkl'\n",
    "ENSG_ENTREZ_MAP  = 'data/ens_entrez_maps/ensg_entrez_mapping.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load STRING database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_data = pd.read_csv(STRING_DATA, delimiter=' ')\n",
    "string_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load HuRI database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huri_data = pd.read_csv(HURI_DATA, delimiter='\\t', header=None)\n",
    "huri_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load duplicate gene data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_gens = pd.read_csv(PARALOG_DATA, delimiter='\\t')\n",
    "dup_gens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to obtain paralog file: awk -F '[=|\\t]' '{if ($1 ~ /HUMAN/ && $11 == \"P\") print $5, $10}' file > output\n",
    "# Use the above bash code to extract the paralogs list from the Panther database\n",
    "# Then convert to entrez gene ids using the gene_id_map below\n",
    "# This file is also included in the supplementary files as gene_map.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to update the PANTHER paralog list to only include actual gene paralogs (and not species etc.)\n",
    "\n",
    "wrong = []\n",
    "for GeneID, uniprot in tqdm(panther_map.iterrows()):\n",
    "    if type(uniprot) != str:\n",
    "        uniprot = uniprot.values[0]\n",
    "    exists = False\n",
    "    for i in paralogs.loc[paralogs['paralog'] == uniprot, 'gene'].append(paralogs.loc[paralogs['gene'] == uniprot, 'paralog']):\n",
    "        if i != uniprot and i in panther_map.uniprot.values:\n",
    "            exists = True\n",
    "            break\n",
    "    if not exists:\n",
    "        wrong.append(uniprot)\n",
    "len(set(wrong))\n",
    "# panther_map.loc[~panther_map.uniprot.isin(wrong)].to_csv(PANTHER_MAP, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorry if the following is a bit messy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Used to create GeneInfo mapping ###\n",
    "gen_inf = 'data/NCBI/gene_info'\n",
    "ens_inf = 'data/NCBI/gene2ensembl'\n",
    "\n",
    "gen_inf_df = pd.read_csv(gen_inf, delimiter='\\t')\n",
    "ens_inf_df = pd.read_csv(ens_inf, delimiter='\\t')\n",
    "\n",
    "gene_id_map = pd.merge(gen_inf_df[['GeneID', 'Symbol']], ens_inf_df[['GeneID', 'Ensembl_gene_identifier', 'Ensembl_protein_identifier']], on='GeneID')\n",
    "\n",
    "gene_id_map.to_csv(GENE_INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ens_to_entrez(ensID):\n",
    "    if ensID in ens_id_map.index:\n",
    "        entrez = ens_id_map.loc[ensID]\n",
    "    else:\n",
    "        entrez = ens_id_map.loc[ens_id_map.index.str.contains(ensID)].values\n",
    "        if entrez.size == 0:\n",
    "            if '.' in ensID:\n",
    "                print(ensID)\n",
    "                entrez = ens_to_entrez(ensID.split('.')[0])\n",
    "            else:\n",
    "                not_in_db.append(ensID)\n",
    "                entrez = -1\n",
    "    try:\n",
    "        entrez = int(entrez)\n",
    "    except Exception as e:\n",
    "        print(ensID)\n",
    "        print(entrez)\n",
    "        entrez = int(entrez[0])\n",
    "        \n",
    "    return entrez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_gens = pd.read_csv(PARALOG_DATA, delimiter='\\t')\n",
    "\n",
    "ens_ids = np.unique(dup_gens['ENS_ID'].values)\n",
    "ens_id_map = gene_id_map.set_index('Ensembl_gene_identifier')['GeneID'].groupby(level=0).first()\n",
    "not_in_db = []\n",
    "\n",
    "for ens_id in tqdm(ens_ids):\n",
    "    if ens_id not in ensg_entrez_map:\n",
    "        ensg_entrez_map[ens_id] = ens_to_entrez(ens_id)\n",
    "        \n",
    "# with open(ENSG_ENTREZ_MAP, 'wb') as f:\n",
    "#     pkl.dump(ensg_entrez_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huri_data = pd.read_csv(HURI_DATA, delimiter='\\t', header=None).values\n",
    "\n",
    "ens_ids = np.unique(huri_data.flatten())\n",
    "ens_id_map = gene_id_map.set_index('Ensembl_gene_identifier')['GeneID'].groupby(level=0).first()\n",
    "not_in_db = []\n",
    "\n",
    "for ens_id in tqdm(ens_ids):\n",
    "    if ens_id not in ensg_entrez_map:\n",
    "        ensg_entrez_map[ens_id] = ens_to_entrez(ens_id)        \n",
    "        \n",
    "with open('not_in_db.txt', 'w') as f:\n",
    "    f.writelines(\"%s\\n\" % i for i in not_in_db)        \n",
    "    \n",
    "# with open(ENSG_ENTREZ_MAP, 'wb') as f:\n",
    "#     pkl.dump(ensg_entrez_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/ens_entrez_maps/not_in_db_map.pkl', 'rb') as f:\n",
    "    ncbi_ids = pkl.load(f)\n",
    "ncbi_ids\n",
    "ensg_entrez_map.update(ncbi_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_ids = np.unique(prot_interactions[['protein1', 'protein2']].values.flatten())\n",
    "ens_id_map = gene_id_map.set_index('Ensembl_protein_identifier').drop('-')['GeneID']\n",
    "not_in_db = []\n",
    "\n",
    "for ens_id in tqdm(ens_ids):\n",
    "    if ens_id not in ensp_entrez_map:\n",
    "        ensp_entrez_map[ens_id] = ens_to_entrez(ens_id)\n",
    "        \n",
    "with open('not_in_db.txt', 'w') as f:\n",
    "    f.writelines(\"%s\\n\" % i for i in not_in_db)\n",
    "\n",
    "# with open(ENSP_ENTREZ_MAP, 'wb') as f:\n",
    "#     pkl.dump(ensp_entrez_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to achieve the SIGNOR interactions in Entrez GeneID format the following was done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The possible protein complexes were split into individual interactions (rows)\n",
    "\n",
    "# See: https://stackoverflow.com/questions/12680754/split-explode-pandas-dataframe-string-entry-to-separate-rows\n",
    "def explode(df, lst_cols, fill_value='', preserve_index=False):\n",
    "    # make sure `lst_cols` is list-alike\n",
    "    if (lst_cols is not None and len(lst_cols) > 0 and \n",
    "        not isinstance(lst_cols, (list, tuple, np.ndarray, pd.Series))):\n",
    "        lst_cols = [lst_cols]\n",
    "    # all columns except `lst_cols`\n",
    "    idx_cols = df.columns.difference(lst_cols)\n",
    "    # calculate lengths of lists\n",
    "    lens = df[lst_cols[0]].str.len()\n",
    "    # preserve original index values    \n",
    "    idx = np.repeat(df.index.values, lens)\n",
    "    # create \"exploded\" DF\n",
    "    res = (pd.DataFrame({\n",
    "                col: np.repeat(df[col].values, lens)\n",
    "                for col in idx_cols},\n",
    "                index=idx)\n",
    "             .assign(**{col: np.concatenate(df.loc[lens>0, col].values)\n",
    "                            for col in lst_cols}))\n",
    "    # append those rows that have empty lists\n",
    "    if (lens == 0).any():\n",
    "        # at least one list in cells is empty\n",
    "        res = (res.append(df.loc[lens==0, idx_cols], sort=False)\n",
    "                  .fillna(fill_value))\n",
    "    # revert the original index order\n",
    "    res = res.sort_index()\n",
    "    # reset index if requested\n",
    "    if not preserve_index:        \n",
    "        res = res.reset_index(drop=True)\n",
    "    return res \n",
    "\n",
    "def split_complexes(df, cols, sep='/'):\n",
    "    df = df.assign(**{c: df[c].str.split(sep) for c in cols})\n",
    "    for col in cols:\n",
    "        df = explode(df, [col])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENTITYA</th>\n",
       "      <th>IDA</th>\n",
       "      <th>IDB</th>\n",
       "      <th>ENTITYB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PDPK1</td>\n",
       "      <td>O15530</td>\n",
       "      <td>O15530</td>\n",
       "      <td>PDPK1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PHLPP1</td>\n",
       "      <td>O60346</td>\n",
       "      <td>P31749</td>\n",
       "      <td>AKT1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OXGR1</td>\n",
       "      <td>Q96P68</td>\n",
       "      <td>P08754</td>\n",
       "      <td>GNAI3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACVR1B</td>\n",
       "      <td>P36896</td>\n",
       "      <td>P69905</td>\n",
       "      <td>HBA1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GPR119</td>\n",
       "      <td>Q8TDV5</td>\n",
       "      <td>P08754</td>\n",
       "      <td>GNAI3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18064</th>\n",
       "      <td>NEDD4L</td>\n",
       "      <td>Q96PU5</td>\n",
       "      <td>Q9UI33</td>\n",
       "      <td>SCN11A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18065</th>\n",
       "      <td>BRCA1</td>\n",
       "      <td>P38398</td>\n",
       "      <td>P07339</td>\n",
       "      <td>CTSD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18066</th>\n",
       "      <td>CREB1</td>\n",
       "      <td>P16220</td>\n",
       "      <td>P98177</td>\n",
       "      <td>FOXO4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18067</th>\n",
       "      <td>P2RY13</td>\n",
       "      <td>Q9BPV8</td>\n",
       "      <td>P30679</td>\n",
       "      <td>GNA15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18068</th>\n",
       "      <td>P2RY2</td>\n",
       "      <td>P41231</td>\n",
       "      <td>Q03113</td>\n",
       "      <td>GNA12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18069 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ENTITYA     IDA     IDB ENTITYB\n",
       "0       PDPK1  O15530  O15530   PDPK1\n",
       "1      PHLPP1  O60346  P31749    AKT1\n",
       "2       OXGR1  Q96P68  P08754   GNAI3\n",
       "3      ACVR1B  P36896  P69905    HBA1\n",
       "4      GPR119  Q8TDV5  P08754   GNAI3\n",
       "...       ...     ...     ...     ...\n",
       "18064  NEDD4L  Q96PU5  Q9UI33  SCN11A\n",
       "18065   BRCA1  P38398  P07339    CTSD\n",
       "18066   CREB1  P16220  P98177   FOXO4\n",
       "18067  P2RY13  Q9BPV8  P30679   GNA15\n",
       "18068   P2RY2  P41231  Q03113   GNA12\n",
       "\n",
       "[18069 rows x 4 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signor_interactions = signor_data[['IDA', 'ENTITYA', 'IDB', 'ENTITYB']].drop_duplicates()\n",
    "signor_interactions = split_complexes(signor_interactions, ['ENTITYA', 'ENTITYB'])\n",
    "signor_interactions\n",
    "# interactions = set()\n",
    "# for _, row in tqdm(signor_interactions.iterrows(), total=len(signor_interactions)):\n",
    "#     if row.ENTITYA in name_id_map.index and row.ENTITYB in name_id_map.index:\n",
    "#         interactions.update([(min(i,j), max(i,j)) for i in name_id_map.loc[row.ENTITYA].values for j in name_id_map.loc[row.ENTITYB].values])\n",
    "# print(len(interactions))\n",
    "# interactions = set(interactions)\n",
    "# print(len(interactions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following are some experiments on converting the gene names to geneIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5696\n",
      "4290\n",
      "0.7531601123595506\n"
     ]
    }
   ],
   "source": [
    "print(len(set(signor_interactions[['ENTITYA', 'ENTITYB']].values.flatten())))\n",
    "print(len(set(signor_interactions[['ENTITYA', 'ENTITYB']].values.flatten()).intersection(gene_id_map.Symbol.values)))\n",
    "\n",
    "print(len(set(signor_interactions[['ENTITYA', 'ENTITYB']].values.flatten()).intersection(gene_id_map.Symbol.values))/len(set(signor_interactions[['ENTITYA', 'ENTITYB']].values.flatten())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5746\n",
      "4282\n",
      "0.7452140619561434\n",
      "\n",
      "22672\n",
      "4019\n",
      "1991\n",
      "\n",
      "23146\n",
      "16683\n",
      "0.7207724876868573\n",
      "16698\n",
      "4561\n"
     ]
    }
   ],
   "source": [
    "# pd.set_option(\"display.max_columns\", 30)\n",
    "print(len(set(signor_interactions[['ENTITYA', 'ENTITYB']].values.flatten())))\n",
    "print(len(set(signor_interactions[['ENTITYA', 'ENTITYB']].values.flatten()).intersection(gene_id_map.Symbol.values)))\n",
    "\n",
    "print(len(set(signor_interactions[['ENTITYA', 'ENTITYB']].values.flatten()).intersection(gene_id_map.Symbol.values))/len(set(signor_interactions[['ENTITYA', 'ENTITYB']].values.flatten())))\n",
    "print()\n",
    "\n",
    "print(len(signor_interactions.loc[(signor_interactions.DATABASEA == 'UNIPROT') | (signor_interactions.DATABASEB == 'UNIPROT')]))\n",
    "print(len(set(signor_interactions.loc[(signor_interactions.DATABASEA == 'UNIPROT') & (signor_interactions.DATABASEB == 'UNIPROT'), ['IDA', 'IDB']].values.flatten())))\n",
    "print(len(set(signor_interactions.loc[(signor_interactions.DATABASEA == 'UNIPROT') & (signor_interactions.DATABASEB == 'UNIPROT'), ['IDA', 'IDB']].values.flatten()).intersection(paralogs.uniprot)))\n",
    "\n",
    "\n",
    "print()\n",
    "# print(len(set(signor_interactions[['ENTITYA', 'ENTITYB']].values.flatten()).intersection([i.split()[0] for i, _ in list(ncbi_gene_names.values())])))\n",
    "# print(signor_interactions.columns)\n",
    "# signor_interactions\n",
    "print(len(signor_interactions))\n",
    "print(len(signor_interactions.loc[(signor_interactions.ENTITYA.isin(gene_id_map.Symbol.values)) & (signor_interactions.ENTITYB.isin(gene_id_map.Symbol.values))]))\n",
    "\n",
    "print(len(signor_interactions.loc[(signor_interactions.ENTITYA.isin(gene_id_map.Symbol.values)) & (signor_interactions.ENTITYB.isin(gene_id_map.Symbol.values))])/len(signor_interactions))\n",
    "\n",
    "print(len(signor_interactions.loc[(signor_interactions.ENTITYA.isin(gene_id_map.Symbol.values) | signor_interactions.IDA.isin(paralogs.uniprot)) & (signor_interactions.ENTITYB.isin(gene_id_map.Symbol.values) | signor_interactions.IDB.isin(paralogs.uniprot))]))\n",
    "print(len(signor_interactions.loc[(signor_interactions.IDA.isin(paralogs.uniprot)) & (signor_interactions.IDB.isin(paralogs.uniprot))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
